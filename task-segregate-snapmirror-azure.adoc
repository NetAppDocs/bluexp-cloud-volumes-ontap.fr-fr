---
sidebar: sidebar 
permalink: task-segregate-snapmirror-azure.html 
keywords: segregate, SnapMirror, SnapMirror traffic, SnapMirror replication, add, additional NIC, new NIC, intercluster LIF, non-routable subnet, subnet 
summary: Avec Cloud Volumes ONTAP dans Azure, vous pouvez isoler le trafic de réplication SnapMirror avec un autre réseau pour améliorer la sécurité et les performances des données. 
---
= Isolez le trafic SnapMirror dans Azure
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Avec Cloud Volumes ONTAP dans Azure, vous pouvez isoler le trafic de réplication SnapMirror du trafic de données et du trafic de gestion. Pour isoler le trafic de réplication SnapMirror de votre trafic de données, vous allez ajouter une nouvelle carte d'interface réseau (NIC), une LIF intercluster associée et un sous-réseau non routable.



== À propos de la ségrégation du trafic SnapMirror dans Azure

Par défaut, BlueXP configure toutes les cartes réseau et LIF d'un déploiement Cloud Volumes ONTAP sur le même sous-réseau. Dans de telles configurations, le trafic de réplication SnapMirror et le trafic de données et de gestion utilisent le même sous-réseau. La ségrégation du trafic SnapMirror exploite un sous-réseau supplémentaire qui n'est pas routable vers le sous-réseau existant utilisé pour le trafic de données et de gestion.

.Figure 1
Les diagrammes suivants illustrent la ségrégation du trafic de réplication SnapMirror avec une carte réseau supplémentaire, une LIF intercluster associée et un sous-réseau non routable dans un déploiement à un seul nœud. Un déploiement de paires haute disponibilité diffère légèrement.

image:diagram-segregate-snapmirror-traffic.png["Le schéma illustre la ségrégation du trafic de réplication SnapMirror dans une configuration à un seul nœud"]

.Avant de commencer
Consultez les considérations suivantes :

* Vous ne pouvez ajouter qu'une seule carte réseau à un seul nœud Cloud Volumes ONTAP ou à un déploiement par paire haute disponibilité (instance de machine virtuelle) pour la ségrégation du trafic SnapMirror.
* Pour ajouter une nouvelle carte réseau, le type d'instance de machine virtuelle que vous déployez doit avoir une carte réseau inutilisée.
* Les clusters source et cible doivent avoir accès au même réseau virtuel (vnet). Le cluster de destination est un système Cloud Volumes ONTAP dans Azure. Le cluster source peut être un système Cloud Volumes ONTAP dans Azure ou un système ONTAP.




== Étape 1 : créez une carte réseau supplémentaire et connectez-la à la machine virtuelle de destination

Cette section fournit des instructions sur la création d'une carte réseau supplémentaire et sa connexion à la machine virtuelle de destination. La machine virtuelle de destination est le système à un seul nœud ou paire haute disponibilité dans Cloud Volumes ONTAP Azure où vous souhaitez configurer votre carte réseau supplémentaire.

.Étapes
. Dans l'interface de ligne de commandes de ONTAP, arrêtez le nœud.
+
[source, cli]
----
dest::> halt -node <dest_node-vm>
----
. Sur le portail Azure, vérifier que l'état de la machine virtuelle (nœud) est arrêté.
+
[source, cli]
----
az vm get-instance-view --resource-group <dest-rg> --name <dest-vm> --query instanceView.statuses[1].displayStatus
----
. Utilisez l'environnement de hachage dans Azure Cloud Shell pour arrêter le nœud.
+
.. Arrêtez le nœud.
+
[source, cli]
----
az vm stop --resource-group <dest_node-rg> --name <dest_node-vm>
----
.. Désallouer le nœud.
+
[source, cli]
----
az vm deallocate --resource-group <dest_node-rg> --name <dest_node-vm>
----


. Configurez les règles du groupe de sécurité réseau de manière à ce que les deux sous-réseaux (sous-réseau de cluster source et sous-réseau de cluster de destination) ne soient pas routables les uns par rapport aux autres.
+
.. Créez la nouvelle carte réseau sur la machine virtuelle de destination.
.. Recherchez l'ID de sous-réseau du sous-réseau du cluster source.
+
[source, cli]
----
az network vnet subnet show -g <src_vnet-rg> -n <src_subnet> --vnet-name <vnet> --query id
----
.. Créez la nouvelle carte réseau sur la machine virtuelle de destination avec l'ID de sous-réseau du sous-réseau de cluster source. Entrez ici le nom de la nouvelle carte réseau.
+
[source, cli]
----
az network nic create -g <dest_node-rg> -n <dest_node-vm-nic-new> --subnet <id_from_prev_command> --accelerated-networking true
----
.. Enregistrez l'adresse IP privée. Cette adresse IP, <new_added_nic_primary_addr>, est utilisée pour créer une LIF intercluster dans <<Step 2: Create a new IPspace,Broadcast domain, LIF intercluster pour le nouveau NIC>>.


. Connectez la nouvelle carte réseau à la machine virtuelle.
+
[source, cli]
----
az vm nic add -g <dest_node-rg> --vm-name <dest_node-vm> --nics <dest_node-vm-nic-new>
----
. Démarrer la machine virtuelle (nœud).
+
[source, cli]
----
az vm start --resource-group <dest_node-rg>  --name <dest_node-vm>
----
. Sur le portail Azure, accédez à *Networking* et vérifiez que la nouvelle carte réseau, par exemple nic-New, existe et que la mise en réseau accélérée est activée.
+
[source, cli]
----
az network nic list --resource-group azure-59806175-60147103-azure-rg --query "[].{NIC: name, VM: virtualMachine.id}"
----


Pour les déploiements de paires haute disponibilité, répétez les étapes pour le nœud partenaire.



== Étape 2 : créer un IPspace, broadcast domain et LIF intercluster pour le nouveau NIC

Un IPspace séparé pour les LIF intercluster assure une séparation logique entre les fonctionnalités de mise en réseau pour la réplication entre les clusters.

Pour les étapes suivantes, utilisez l'interface de ligne de commandes ONTAP.

.Étapes
. Créez le nouvel IPspace (New_ipspace).
+
[source, cli]
----
dest::> network ipspace create -ipspace <new_ipspace>
----
. Créez un broadcast domain sur le nouveau IPspace (New_ipspace) et ajoutez le nic-New port.
+
[source, cli]
----
dest::> network port show
----
. Pour les systèmes à un seul nœud, le nouveau port ajouté est _e0b_. Pour les déploiements de paires haute disponibilité avec des disques gérés, le nouveau port ajouté est _e0d_. Pour les déploiements de paires haute disponibilité avec des blobs de page, le nouveau port ajouté est _e0e_. Utilisez le nom du nœud et non le nom de la machine virtuelle. Recherchez le nom du nœud en exécutant `node show`.
+
[source, cli]
----
dest::> broadcast-domain create -broadcast-domain <new_bd> -mtu 1500 -ipspace <new_ipspace> -ports <dest_node-cot-vm:e0b>
----
. Créer un LIF intercluster sur le nouveau broadcast-domain (New_bd) et sur le nouveau NIC (nic-New)
+
[source, cli]
----
dest::> net int create -vserver <new_ipspace> -lif <new_dest_node-ic-lif> -service-policy default-intercluster -address <new_added_nic_primary_addr> -home-port <e0b> -home-node <node> -netmask <new_netmask_ip> -broadcast-domain <new_bd>
----
. Vérifier la création du nouveau LIF intercluster.
+
[source, cli]
----
dest::> net int show
----


Pour les déploiements de paires haute disponibilité, répétez les étapes pour le nœud partenaire.



== Étape 3 : vérification du peering de cluster entre les systèmes source et destination

Cette section fournit des instructions pour vérifier le peering entre les systèmes source et de destination.

Pour les étapes suivantes, utilisez l'interface de ligne de commandes ONTAP.

.Étapes
. Vérifier que le LIF intercluster du cluster destination peut envoyer une requête ping au LIF intercluster du cluster source. Étant donné que le cluster de destination exécute cette commande, l'adresse IP de destination est l'adresse IP du LIF intercluster sur la source.
+
[source, cli]
----
dest::> ping -lif <new_dest_node-ic-lif> -vserver <new_ipspace> -destination <10.161.189.6>
----
. Vérifier que le LIF intercluster du cluster source peut envoyer une requête ping au LIF intercluster du cluster destination La destination est l'adresse IP de la nouvelle carte réseau créée sur la destination.
+
[source, cli]
----
src::> ping -lif <src_node-ic-lif> -vserver <src_svm> -destination <10.161.189.18>
----


Pour les déploiements de paires haute disponibilité, répétez les étapes pour le nœud partenaire.



== Étape 4 : créer le peering de SVM entre le système source et le système de destination

Cette section fournit des instructions pour la création du peering de SVM entre le système source et le système de destination.

Pour les étapes suivantes, utilisez l'interface de ligne de commandes ONTAP.

.Étapes
. Créer le peering de cluster sur la destination en utilisant l'adresse IP du LIF intercluster source en tant que `-peer-addrs`. Pour les paires HA, lister l'adresse IP du LIF intercluster source pour les deux nœuds comme `-peer-addrs`.
+
[source, cli]
----
dest::> cluster peer create -peer-addrs <10.161.189.6> -ipspace <new_ipspace>
----
. Saisissez et confirmez la phrase de passe.
. Créer le peering de cluster sur la source en utilisant l'adresse IP de la LIF du cluster de destination comme `peer-addrs`. Pour les paires HA, lister l'adresse IP du LIF intercluster de destination pour les deux nœuds comme `-peer-addrs`.
+
[source, cli]
----
src::> cluster peer create -peer-addrs <10.161.189.18>
----
. Saisissez et confirmez la phrase de passe.
. Vérifier que le cluster a bien été peering.
+
[source, cli]
----
src::> cluster peer show
----
+
Le peering réussi affiche *disponible* dans le champ de disponibilité.

. Créer le peering de SVM sur la destination. Les SVM source et destination doivent être des SVM de données.
+
[source, cli]
----
dest::> vserver peer create -vserver <dest_svm> -peer-vserver <src_svm> -peer-cluster <src_cluster> -applications snapmirror``
----
. Accepter le peering de SVM.
+
[source, cli]
----
src::> vserver peer accept -vserver <src_svm> -peer-vserver <dest_svm>
----
. Vérifier que le SVM a bien peering.
+
[source, cli]
----
dest::> vserver peer show
----
+
L'état homologue s'affiche *`peered`* et les applications de peering montrent *`snapmirror`*.





== Étape 5 : création d'une relation de réplication SnapMirror entre le système source et le système de destination

Cette section fournit des instructions sur la création d'une relation de réplication SnapMirror entre le système source et le système de destination.

Pour déplacer une relation de réplication SnapMirror existante, vous devez d'abord interrompre la relation de réplication SnapMirror existante avant de créer une nouvelle relation de réplication SnapMirror.

Pour les étapes suivantes, utilisez l'interface de ligne de commandes ONTAP.

.Étapes
. Créer un volume protégé des données sur le SVM de destination
+
[source, cli]
----
dest::> vol create -volume <new_dest_vol> -vserver <dest_svm> -type DP -size <10GB> -aggregate <aggr1>
----
. Créer la relation de réplication SnapMirror sur la destination qui inclut la règle SnapMirror et la planification de la réplication.
+
[source, cli]
----
dest::> snapmirror create -source-path src_svm:src_vol  -destination-path  dest_svm:new_dest_vol -vserver dest_svm -policy MirrorAllSnapshots -schedule 5min
----
. Initialiser la relation de réplication SnapMirror sur la destination
+
[source, cli]
----
dest::> snapmirror initialize -destination-path  <dest_svm:new_dest_vol>
----
. Dans l'interface de ligne de commandes de ONTAP, valider le statut de la relation SnapMirror en exécutant la commande suivante :
+
[source, cli]
----
dest::> snapmirror show
----
+
L'état de la relation est `Snapmirrored` et la santé de la relation est `true`.

. Facultatif : dans l'interface de ligne de commandes de ONTAP, exécutez la commande suivante pour afficher l'historique des actions de la relation SnapMirror.
+
[source, cli]
----
dest::> snapmirror show-history
----


Vous pouvez également monter les volumes source et de destination, écrire un fichier sur la source et vérifier que le volume est répliqué vers la destination.
